"""
Módulo de análise de sentimento financeiro melhorado.
Inclui rate limiting, caching, error handling robusto e múltiplas fontes.
"""
import requests
from bs4 import BeautifulSoup
from transformers import pipeline
import re
import hashlib
from collections import defaultdict
import time
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import yfinance as yf
from functools import lru_cache
import json

from config import settings

logger = logging.getLogger(__name__)


class SentimentAnalysisService:
    \"\"\"Serviço de análise de sentimento com cache e rate limiting.\"\"\"\n    \n    def __init__(self):\n        self.cache = {}\n        self.last_request_time = defaultdict(float)\n        self.min_request_interval = 2.0  # segundos entre requests\n        \n        # Inicializar pipeline de NLP\n        try:\n            self.sentiment_analyzer = pipeline(\n                \"sentiment-analysis\",\n                model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n                return_all_scores=True\n            )\n            logger.info(\"Pipeline de sentiment analysis inicializado\")\n        except Exception as e:\n            logger.warning(f\"Erro ao inicializar pipeline NLP: {e}\")\n            self.sentiment_analyzer = None\n        \n        # Configurações de fontes de notícias\n        self.news_sources = {\n            \"investopedia\": {\n                \"url_template\": \"https://www.investopedia.com/search?q={}\",\n                \"parser\": \"html.parser\",\n                \"selectors\": {\n                    \"tag\": \"h3\",\n                    \"attrs\": {\"class\": re.compile(\"heading\")}\n                }\n            },\n            \"yahoo_finance\": {\n                \"url_template\": \"https://finance.yahoo.com/quote/{}/news\",\n                \"parser\": \"html.parser\",\n                \"selectors\": {\n                    \"tag\": \"h3\",\n                    \"attrs\": {\"class\": re.compile(\"title\")}\n                }\n            }\n        }\n    \n    def _rate_limit_check(self, source: str) -> bool:\n        \"\"\"Verifica se é necessário aguardar antes de fazer nova requisição.\"\"\"\n        current_time = time.time()\n        last_time = self.last_request_time[source]\n        \n        if current_time - last_time < self.min_request_interval:\n            wait_time = self.min_request_interval - (current_time - last_time)\n            logger.info(f\"Rate limiting: aguardando {wait_time:.2f}s para {source}\")\n            time.sleep(wait_time)\n        \n        self.last_request_time[source] = time.time()\n        return True\n    \n    def _get_cache_key(self, ticker: str, source: str = \"all\") -> str:\n        \"\"\"Gera chave de cache baseada no ticker e data.\"\"\"\n        date_str = datetime.now().strftime(\"%Y-%m-%d-%H\")\n        return f\"{ticker}_{source}_{date_str}\"\n    \n    def _is_cache_valid(self, cache_key: str) -> bool:\n        \"\"\"Verifica se o cache ainda é válido.\"\"\"\n        if cache_key not in self.cache:\n            return False\n        \n        cached_time = self.cache[cache_key].get(\"timestamp\", 0)\n        current_time = time.time()\n        \n        return (current_time - cached_time) < settings.CACHE_TTL\n    \n    def preprocess_text(self, text: str) -> str:\n        \"\"\"Limpa e padroniza texto.\"\"\"\n        # Remove URLs\n        text = re.sub(r\"http\\S+\", \"\", text)\n        # Remove caracteres especiais\n        text = re.sub(r\"[^A-Za-z0-9À-ÿ.,;!?() ]+\", \" \", text)\n        # Remove espaços extras\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n        return text\n    \n    @lru_cache(maxsize=100)\n    def get_company_name_from_ticker(self, ticker: str) -> str:\n        \"\"\"Obtém nome da empresa a partir do ticker (com cache).\"\"\"\n        ticker = ticker.upper().strip()\n        \n        # Verificar se é ticker brasileiro\n        is_brazilian = bool(re.match(r'^[A-Z]{4}[0-9]$', ticker)) or ticker.endswith('.SA')\n        \n        if is_brazilian and not ticker.endswith('.SA'):\n            ticker += \".SA\"\n        \n        try:\n            stock = yf.Ticker(ticker)\n            info = stock.info\n            return info.get(\"longName\") or info.get(\"shortName\") or ticker\n        except Exception as e:\n            logger.warning(f\"Erro ao buscar nome da empresa para {ticker}: {e}\")\n            return ticker\n    \n    def fetch_news_from_source(self, url: str, source_config: Dict[str, Any], \n                              relevant_terms: List[str]) -> List[Dict[str, str]]:\n        \"\"\"Busca notícias de uma fonte específica.\"\"\"\n        headlines_data = []\n        \n        try:\n            headers = {\n                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n            }\n            \n            response = requests.get(url, headers=headers, timeout=settings.REQUEST_TIMEOUT)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.text, source_config[\"parser\"])\n            selectors = source_config[\"selectors\"]\n            \n            for element in soup.find_all(selectors[\"tag\"], selectors[\"attrs\"]):\n                headline_text = self.preprocess_text(element.get_text())\n                \n                # Verificar relevância\n                if (len(headline_text) > 30 and \n                    any(term.lower() in headline_text.lower() for term in relevant_terms)):\n                    \n                    # Tentar extrair conteúdo adicional\n                    article_content = self._extract_article_content(element)\n                    \n                    text_for_analysis = headline_text\n                    if article_content and len(article_content) > 50:\n                        text_for_analysis = f\"{headline_text}. {article_content}\"\n                    \n                    headlines_data.append({\n                        \"text\": text_for_analysis,\n                        \"source_url\": url,\n                        \"headline\": headline_text\n                    })\n        \n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Erro de requisição em {url}: {e}\")\n        except Exception as e:\n            logger.error(f\"Erro inesperado ao buscar em {url}: {e}\")\n        \n        return headlines_data\n    \n    def _extract_article_content(self, element) -> str:\n        \"\"\"Extrai conteúdo adicional do artigo.\"\"\"\n        try:\n            parent = element.find_parent([\"div\", \"article\"])\n            if parent:\n                # Buscar resumo ou primeiro parágrafo\n                summary_elem = parent.find(\"p\", class_=re.compile(\"summary|description|resumo\"))\n                \n                if not summary_elem:\n                    content_div = parent.find(\"div\", class_=re.compile(\"body|content\"))\n                    if content_div:\n                        summary_elem = content_div.find(\"p\")\n                \n                if summary_elem:\n                    return self.preprocess_text(summary_elem.get_text())\n        except Exception:\n            pass\n        \n        return \"\"\n    \n    def deduplicate_headlines(self, headlines_data: List[Dict[str, str]]) -> List[Dict[str, str]]:\n        \"\"\"Remove duplicatas usando hash.\"\"\"\n        unique_headlines = []\n        seen_hashes = set()\n        \n        for item in headlines_data:\n            headline_hash = hashlib.md5(item['text'].encode('utf-8')).hexdigest()\n            if headline_hash not in seen_hashes:\n                unique_headlines.append(item)\n                seen_hashes.add(headline_hash)\n        \n        return unique_headlines\n    \n    def filter_financial_relevance(self, headlines_data: List[Dict[str, str]], \n                                  primary_term: str) -> List[Dict[str, str]]:\n        \"\"\"Filtra notícias por relevância financeira.\"\"\"\n        financial_keywords = [\n            \"earnings\", \"revenue\", \"profit\", \"stock\", \"shares\", \"market\", \"trading\",\n            \"investment\", \"investor\", \"financial\", \"quarter\", \"quarterly\", \"annual\",\n            \"lucro\", \"receita\", \"ação\", \"ações\", \"mercado\", \"investimento\", \"investidor\",\n            \"financeiro\", \"trimestre\", \"anual\", \"bolsa\", \"economia\"\n        ]\n        \n        irrelevant_keywords = [\n            \"sports\", \"celebrity\", \"entertainment\", \"weather\", \"politics\",\n            \"esporte\", \"celebridade\", \"entretenimento\", \"tempo\", \"política\"\n        ]\n        \n        filtered = []\n        \n        for item in headlines_data:\n            text_lower = item['text'].lower()\n            \n            # Verificar se contém termos financeiros\n            has_financial_context = any(keyword in text_lower for keyword in financial_keywords)\n            \n            # Verificar se não contém termos irrelevantes\n            has_irrelevant_context = any(keyword in text_lower for keyword in irrelevant_keywords)\n            \n            # Verificar se menciona o termo principal\n            has_primary_term = primary_term.lower() in text_lower\n            \n            if (has_financial_context or has_primary_term) and not has_irrelevant_context:\n                filtered.append(item)\n        \n        return filtered\n    \n    def analyze_sentiment_with_transformers(self, headlines_data: List[Dict[str, str]]) -> Dict[str, Any]:\n        \"\"\"Analisa sentimento usando transformers.\"\"\"\n        if not self.sentiment_analyzer or not headlines_data:\n            return self._fallback_sentiment_analysis(headlines_data)\n        \n        try:\n            # Combinar todos os textos\n            combined_text = \" \".join([item['text'] for item in headlines_data[:20]])  # Limite\n            \n            # Analisar sentimento\n            results = self.sentiment_analyzer(combined_text[:512])  # Limite do modelo\n            \n            if results and len(results[0]) > 0:\n                # Encontrar label com maior score\n                best_result = max(results[0], key=lambda x: x['score'])\n                \n                # Mapear para nosso formato\n                sentiment_mapping = {\n                    \"POSITIVE\": \"positivo\",\n                    \"NEGATIVE\": \"negativo\",\n                    \"1\": \"muito negativo\",\n                    \"2\": \"negativo\",\n                    \"3\": \"neutro\",\n                    \"4\": \"positivo\",\n                    \"5\": \"muito positivo\"\n                }\n                \n                sentiment_label = sentiment_mapping.get(\n                    best_result['label'], \n                    \"neutro\"\n                )\n                \n                return {\n                    \"final_sentiment\": sentiment_label,\n                    \"confidence\": best_result['score'],\n                    \"sources_count\": len(headlines_data),\n                    \"method\": \"transformers\"\n                }\n        \n        except Exception as e:\n            logger.error(f\"Erro na análise com transformers: {e}\")\n        \n        return self._fallback_sentiment_analysis(headlines_data)\n    \n    def _fallback_sentiment_analysis(self, headlines_data: List[Dict[str, str]]) -> Dict[str, Any]:\n        \"\"\"Análise de sentimento de fallback usando palavras-chave.\"\"\"\n        if not headlines_data:\n            return {\n                \"final_sentiment\": \"neutro\",\n                \"confidence\": 0.0,\n                \"sources_count\": 0,\n                \"method\": \"fallback\"\n            }\n        \n        positive_words = [\n            \"growth\", \"profit\", \"gain\", \"rise\", \"increase\", \"strong\", \"beat\", \"bullish\",\n            \"crescimento\", \"lucro\", \"ganho\", \"alta\", \"aumento\", \"forte\", \"supera\"\n        ]\n        \n        negative_words = [\n            \"loss\", \"decline\", \"fall\", \"decrease\", \"weak\", \"miss\", \"bearish\", \"drop\",\n            \"perda\", \"declínio\", \"queda\", \"diminuição\", \"fraco\", \"não atinge\"\n        ]\n        \n        positive_score = 0\n        negative_score = 0\n        \n        for item in headlines_data:\n            text_lower = item['text'].lower()\n            \n            positive_score += sum(1 for word in positive_words if word in text_lower)\n            negative_score += sum(1 for word in negative_words if word in text_lower)\n        \n        total_score = positive_score + negative_score\n        \n        if total_score == 0:\n            sentiment = \"neutro\"\n            confidence = 0.5\n        else:\n            positive_ratio = positive_score / total_score\n            \n            if positive_ratio > 0.6:\n                sentiment = \"positivo\"\n                confidence = positive_ratio\n            elif positive_ratio < 0.4:\n                sentiment = \"negativo\"\n                confidence = 1 - positive_ratio\n            else:\n                sentiment = \"neutro\"\n                confidence = 0.5\n        \n        return {\n            \"final_sentiment\": sentiment,\n            \"confidence\": confidence,\n            \"sources_count\": len(headlines_data),\n            \"method\": \"keyword_based\"\n        }\n    \n    def enhanced_sentiment_analysis(self, ticker: str) -> Dict[str, Any]:\n        \"\"\"Análise de sentimento principal com cache.\"\"\"\n        # Verificar cache\n        cache_key = self._get_cache_key(ticker)\n        \n        if self._is_cache_valid(cache_key):\n            logger.info(f\"Retornando sentiment em cache para {ticker}\")\n            return self.cache[cache_key][\"result\"]\n        \n        try:\n            # Obter nome da empresa\n            company_name = self.get_company_name_from_ticker(ticker)\n            relevant_terms = [ticker, company_name]\n            \n            all_headlines = []\n            \n            # Buscar notícias de múltiplas fontes\n            for source_name, source_config in self.news_sources.items():\n                try:\n                    self._rate_limit_check(source_name)\n                    \n                    url = source_config[\"url_template\"].format(ticker)\n                    headlines = self.fetch_news_from_source(url, source_config, relevant_terms)\n                    \n                    if headlines:\n                        all_headlines.extend(headlines)\n                        logger.info(f\"Obtidas {len(headlines)} notícias de {source_name}\")\n                \n                except Exception as e:\n                    logger.warning(f\"Erro ao buscar notícias de {source_name}: {e}\")\n            \n            # Processar notícias\n            if all_headlines:\n                unique_headlines = self.deduplicate_headlines(all_headlines)\n                filtered_headlines = self.filter_financial_relevance(unique_headlines, ticker)\n                \n                if filtered_headlines:\n                    result = self.analyze_sentiment_with_transformers(filtered_headlines)\n                else:\n                    result = {\"final_sentiment\": \"neutro\", \"confidence\": 0.0, \"sources_count\": 0}\n            else:\n                result = {\"final_sentiment\": \"neutro\", \"confidence\": 0.0, \"sources_count\": 0}\n            \n            # Cache do resultado\n            self.cache[cache_key] = {\n                \"result\": result,\n                \"timestamp\": time.time()\n            }\n            \n            logger.info(f\"Análise de sentimento para {ticker}: {result['final_sentiment']}\")\n            return result\n        \n        except Exception as e:\n            logger.error(f\"Erro na análise de sentimento para {ticker}: {e}\")\n            return {\"final_sentiment\": \"neutro\", \"confidence\": 0.0, \"sources_count\": 0, \"error\": str(e)}\n\n\n# Instância global do serviço\nsentiment_service = SentimentAnalysisService()\n\n\n# Função de compatibilidade\ndef enhanced_sentiment_analysis(ticker: str) -> str:\n    \"\"\"Função de compatibilidade que retorna apenas o sentiment.\"\"\"\n    try:\n        result = sentiment_service.enhanced_sentiment_analysis(ticker)\n        return result.get(\"final_sentiment\", \"neutro\")\n    except Exception as e:\n        logger.error(f\"Erro na análise de sentimento para {ticker}: {e}\")\n        return \"neutro\""